
As mentioned in Chapter \ref{chap:CBM_STS}, \gls{CBM} will face an unprecedented interaction rate in heavy-ion experiments (up to $10^{7}$~events/s). That number also sets a clear requirement for the detector systems and their corresponding data acquisition. Certain design decisions had to be made to reduce the amount of raw data coming from the subsystems due to the huge quantity of incoming data.

Experiments like \gls{CMS}, use a trigger to \cite{CMS-DAQ} reduce the amount of raw data coming from millions of proton-proton collisions.  In the case of \gls{CBM}, to reduce the data amount, the raw data needs to be evaluated in software on Central Processing Unit (\gls{CPU}) and/or Graphics Processing Unit (\gls{GPU}) level. The self-triggered readout system implies that the association of data from different detectors to individual physical collision events must be based solely on their timestamp, which is generated in the frond-end electronic (\gls{FEE}) circuitry. As a result, a central timing system must synchronize the \gls{FEE} elements to sub-nanosecond precision. On the other hand, the typical event-building action and the high-level trigger are transitioned to the \gls{FLES} (First-level Event Selector) online computing farm. 


\begin{figure}[!h]
\centering
\includegraphics[width=0.7\columnwidth]{Chapter3/Controls/images/online.png}
\caption{General schematics of the CBM readout systems without detector detector specific systems.}
\label{fig_controls}
\end{figure}

\newpage
The readout hardware is connected to the computing farm via custom-developed optical links that manage clock and time distribution, data transfer, and control communication. The Common Readout Interface (\gls{CRI}) connects the links to the online farm. The \gls{CRI} forwards the clock and time information obtained from the Timing and Fast Control (\gls{TFC}) system to the detector \gls{FEE}, and also converts the data received from the detector. Figure~\ref{fig_controls} shows the schematic view of the controls and data acquisition chain of the \gls{CBM} experiment. The Experiment Control System (\gls{ECS}) highlighted in red is also the supervisory structure of the Detector Control System (\gls{DCS}) which controls and monitors the subsystems. The next sections focus on the software components related to the \gls{ECS}, and a detailed explanation of the experiment control with its design. The main focus of the recent work is put on the \gls{DCS}. An introduction to the modern control system frameworks is given, together with a detailed explanation of the functionalities of the specific software components. 
%\newpage
\section{Controlling the CBM experiment}

Heavy-ion physics experiments require complex control systems, which are crucial to the successful operation of the detector system. Proper implementation of such systems ensures an understanding of the safety margins and enhanced data production quality. In general, the whole system should be robust, partitioned, modular, distributed, and highly available. Similar topics were also considered while designing the \gls{STS} control system.

Figure \ref{fig_sim} depicts the targeted control architecture of the future \gls{CBM} experiment. It consists of different software agents\footnote{A software agent is a persistent, goal-oriented computer program.} with clearly defined tasks. During the Phase - 0 experiment of the \gls{CBM} (\gls{mCBM}) some parts of the future \gls{ECS} were tested. The respective parts of the controls have been tested in a standalone mode, which means that there has not been any structured communication between \gls{DCS}, Device Control Agent (\gls{DCA}), and Experiment Control System (\gls{ECS}). Nevertheless, for the final experiment, the detector control system should provide the information on the detector state to the agents residing at a higher level in the control hierarchy and also request the state of the~\gls{DAQ}.

%\newpage
\begin{figure}[!h]
\centering
\includegraphics[width=0.8\columnwidth]{Chapter3/Controls/images/AgentsRelations_V2.pdf}
\caption{\gls{ECS} core agent relations. The numbers and letters indicate how many instances of agents or systems can run concurrently.}
\label{fig_sim}
\end{figure}

 In the next sections, the main features of the control agents are discussed, in particular those that can influence \gls{DCS} (Partition Control Agent (\gls{PCA}), System Control Agent (\gls{SCA}), and \gls{DCA}). Apart from the mentioned agents, the following components are expected to be the part of the experiment controls architecture: 
 \begin{itemize}
     \item Logging and monitoring system - state or configuration changes should be documented for possible revision.
     \item First Level Event Selector network (FLESnet) - \gls{DAQ} software controlling the data readout and timeslice building.
     \item Online processing - software receiving the data from the FLESnet and processing it,
     \item Time and Fast Control (\gls{TFC}) - hardware source of timing information.
 \end{itemize}
\section{Experiment Control System and its structure}\label{sssAgents}

The highest supervisory element of the control strategy is the Experiment Control Agent (\gls{ECA}). It is the top layer of the \gls{ECS}, which should be constantly running and keeping track of the  partitions, the systems in partitions, and the systems out of partitions. 


The \gls{PCA}, is the bottom layer of the \gls{ECS}, which tracks the state and controls a set of detector systems plus all needed central systems. Multiple partitions can run concurrently, potentially allowing for parallel runs with independent detector system sets. Partition (controlled by \gls{PCA}) is a component of the \gls{ECS} describing the combined state of a set of detector systems participating in a common readout. It is used to segment the readout and data flow. It also manages the states of central systems, which do not have partition-level states (e.g. \gls{TFC}, \gls{FLES} in case it does not use internal partitions).  It provides the address and port required by agents to establish the 0MQ\footnote{0MQ is an asynchronous messaging library \cite{zeromq}.} sockets\footnote{A socket is used by a client to send requests to and receive replies from a service.} and build the \gls{ECS} structure upon request.

The \glspl{PCA} hold internal instances of the necessary System Control Agent (\gls{SCA}) interfaces, which are responsible for:
\begin{itemize}
 \item Holding a copy of the current state of the \gls{SCA}.
 \item Periodic ping of the \gls{SCA} to ensure early disconnection detection.
 \item Sending requests to the \gls{SCA} and receiving the replies.
 \item Monitoring the \gls{SCA} broadcast channel for unexpected state changes.
\end{itemize}

These should not be confused with the \gls{SCA}, which are in separate processes (the list will be dynamic and generic at \gls{PCA} level, and can depend on the systems participating in the readout activities): \gls{BMON}, \gls{MVD}, \gls{STS}, \gls{RICH}, \gls{MUCH}, \gls{TRD}, \gls{TOF}, \gls{PSD}. 

The SCA is the first detector supervisory layer, which contains the current information about the state of the \gls{DAQ} and \gls{DCS}. Its role in division into the underlying systems is discussed in the next subsection.

\subsection{System Control Agent and its role}
The subsystem-specific \gls{SCA} should manage communication both with the detector-specific agents and with the higher supervisory entities of \gls{ECS}. Furthermore, \gls{SCA} should provide higher control levels with the global state whenever it is requested, handle any changes reported by a detector (tracking the state), and provide an interface for the shift crew to monitor the state of the system. 

Two main links of the \gls{SCA} include:
\begin{itemize}
    \item Detector Control System - Experimental Physics and Industrial Control System \gls{EPICS}~\cite{EPICS} based distributed control system which controls and monitors all the hardware connected to the specific detector (apart from the \gls{DAQ} specific boards).
    \item Device Control Agent (\gls{DCA}) - this agent controls almost all logic on the \gls{CRI} board (excluding the FLES Interface Module (\gls{FLIM}) section and direct memory access data path). \gls{DCA} provides a high-level interface for the higher layers of the control architecture, collectively referred to as \gls{EDC}. 
\end{itemize}











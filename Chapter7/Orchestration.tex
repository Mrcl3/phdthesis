\subsection{Outlook for the DCS software services} 
Software applications introduced in the thesis proved to be extremely useful for small and medium size setups (up to a few thousand \glspl{PV}. Nevertheless, for the final experiment, a more sophisticated software infrastructure is needed.

The final system will feature an orchestrator which should be scalable, highly available, provide logging, and monitoring capabilities, and be redundant. Orchestration leads to automation of the container deployment, as well as to balance the workload. Orchestrators can:
 \begin{itemize}
     \item Automatically deploy containers based on policies, application load, and environmental metrics.
     \item Identify failed containers or clusters and heal them.
     \item Manage application configuration.
     \item Connect containers to storage and manage networking.
     \item Improve security by restricting access between containers, and between containers and external systems.
 \end{itemize}

 Examples of orchestrators include DockerSwarm~\cite{DockerSwarm}, and Kubernetes \cite{Kubernetes}. One of the recent applications of Kubernetes was reported in \cite{ICALEPCS2021:Diamond}, where the whole test beam line is operated with containers. Similarly, the whole \gls{CBM} \gls{DCS} could be operated with containers and orchestration tools. 

 Apart from the orchestration, the \gls{DCS} should be supplemented with:
 \begin{itemize}
      \item Gateway(s) - considering the low voltage powering of the \gls{STS} \gls{FEE}, which consists of about 2100 low voltage channels, 140 modules, and 14 crates. Each crate has a controller with an embedded \gls{IOC}, publishing all the process variables. By putting the power supplies into a different subnet or network, it is possible toe easily debug potential problems and limit the network traffic. That is why it has been endorsed to use CA Gateway \cite{gateway} or \gls{PV} gateway, which will take care of regulating access between the subnets in the DCS network. It also provides additional access security, assuring that the \glspl{IOC} running the key services like powering run smoothly.
     \item Time synchronization - as mentioned in Section \ref{archiver}, the archiver could be split into a few nodes serving as temporary data storage (short-term, mid-term). Proper daily backup to GSI managed database would be recommended, but it depends mostly on the database services provided by the GSI IT. So far, the Redis DB has been used, but also other options should be considered.       
     \item Logbook - one of the missing elements in the \gls{mSTS} architecture is a logbook. So far, for all the mCBM-related activities, elog \cite{elog} was used. For the final experiment, a dedicated elog branch will be implemented and the elog client will be used \cite{elog_client},
     \item Save and restore service - it is organized twofold. A tool, called autosave, which is a part of the synApps module \cite{autosave} preserves \glspl{PV} values through the \gls{IOC} reboot. The second set of tools that permits taking snapshots and saving configurations is MASAR \cite{masar}. It is a more complex tool than autosave, offering also a Phoebus-based \gls{GUI}.
     \item Data persistence - to properly archive and analyze the data, it is necessary that all nodes, \glspl{IOC}, and other software applications are synchronized. As it is impossible to adjust the clock in the containers, it should be synchronized on each node separately. The central \gls{DCS} node will provide a Network Time Protocol (\gls{NTP}) daemon that will be synchronized with one of the public, official sources.  By doing so, the clocks of all the containers running control applications will be automatically synchronized even if the external network connection is not available.
     \item Communication protocol - it was reported during the EPICS Collaboration meeting 2022 \cite{epics_2022} that the transition to the newer protocol (PV access) is ongoing. Nevertheless, CA and PVA are both included in the EPICS 7, which should be the base image of the \gls{CBM} \gls{IOC} image, and also for the next versions of the \gls{IOC}. PVA is under constant development and will offer even more features in the coming years, thus for the future CBM experiment (timeline of more than 10 years), it is an optimal choice. 
 \end{itemize}


\subsection{Failover considerations}
The high availability of services plays a key role in the safe operation of a detector. Once all the services are deployed, only short breaks are foreseen during 10 years of operation. The STS has to be constantly cooled, in order to avoid performance degradation of the silicon sensors and \gls{FEE}.

Crucial elements of the STS, like the air drying plant or the cooling plant, will be monitored and controlled by a \gls{PLC}-based system. This hardware layer will provide essential safety measures in case of failure. The \gls{PLC} will also be linked to an \gls{IOC} publishing the values to the software layer. 

Even before triggering the hardware interlock, any potentially hazardous system behavior will be discovered by the software of the control system. In order to ensure maximum safety, failover mechanisms will be exercised to mitigate potential \gls{IOC} failures. There are two considered methods to address failover:
\begin{enumerate}
    \item Considering a scenario in which the hardware is controlled, e.g., via a network. In that case, it is possible to deploy a backup \gls{IOC}, which has the same configuration as the main \gls{IOC}, therefore providing a replacement if it fails. Under some standards, like example RS232, it is not considered good practice to have two \glspl{IOC} connected to the same node. Furthermore, in the case of RS232 a multiplexer would be needed to implement a redundant solution. 
    \item A second possibility is to use failover mechanisms based on an orchestrator, in this case, the deployment and life cycle of a container is governed by an additional tool, i.e., Kubernetes. In case one of the containers (\glspl{IOC}) hangs up, it will be automatically stopped, and a new container will take over the tasks. Nevertheless, the newly deployed container could have a different configuration, therefore changing the state of the whole system. 
\end{enumerate}




%\section{Failure considerations}

%Failure considerations are an essential aspect of designing a detector and its services. Some systems might need to be planned as redundant structures that automatically bring a detector into a safe state in case of an issue and/or failure. For \gls{STS} there will be a few layers of dependencies and interlocks, including the software and hardware ones.

%Low voltage power supply failures can be divided into channel, module, controller, or crate failures. The example below describes the worst-case scenario in which the failed low-voltage module was connected only to the readout boards.
%\begin{itemize}
%    \item channel failure - in case of a permanent failure, up to 5 %\glspl{FEB} might be off (if that channel powers a \gls{ROB}),
%    \item module failure - up to 75 \glspl{FEB} might get disabled (4.3\% %of all boards), 
%    \item crate failure (10 \gls{LV} modules in the crate) - up to 750 %boards stop working, which constitutes to about 43\% of the STS.
%\end{itemize}

%As the LV power supplies will be located in the array which is not accessible during the beam time, any kind of permanent failure will result in a loss of the physics data. 

%Another type of failure that is dangerous for the system is an uncontrolled increase of water content inside the detector. This could be caused by the failure of the drying system, loss of confinement, or cooling system failure. 
%\section{Detector safety}


\section{Final remarks}

The scope of the thesis focused on developing a modular control system framework that can be implemented for small, medium, and large experimental setups. This framework was used for setups that required a remote operation, like the irradiation of the powering modules for the \gls{FEE}, but also in laboratory-based setups where the automation and archiving were needed (thermal cycling of the \gls{STS} electronics).

With the help of the \gls{EPICS} related applications, it was found that the low voltage powering module will experience soft errors of up to 9 per month during the \gls{CBM} operation. Such behavior poses a risk to the experiment operation as it could cause deterioration of the physics performance, but also a possible danger to the \gls{FEE}. On the other hand, the \gls{HV} channels would be switched off even more often, but in the case of the \gls{CBM} they are located far away from the experimental site.

It was further assessed what are the limitations of the \glspl{FEB} with respect to the thermal cycling and the mechanical stress that is therefore induced. The results served as an indication of possible failure modes of the \gls{FEB} at the end of \gls{STS} lifetime. Failure modes after repeated cycles, and potential reasons were determined (e.g., \gls{CTE} difference between the materials). 

Another application of the developed framework was related to the testing and characterization of the humidity sensors. A general strategy for ambient parameters monitoring inside the \gls{STS} was developed, and potential sensor candidates were chosen. A sampling system with a ceramic sensor and \gls{FOS} were identified as reliable solutions for the distributed sensing system. Additionally, the industrial capacitive sensors will be used as a reference during the commissioning.

The \gls{FOS} hygrometer turned out to be a more reliable solution in comparison to a sensor array. One of the possible reasons of  worse performance is a relatively low distance between the subsequent sensors and a thicker coating. The results obtained from the time response study pointed out that the thinner coating of about 15\,$\mu m$ should be a good compromise between the humidity sensitivity and the time response. 

Chapter~\ref{chap:msts} focused on the main implementation of the containerized-based control system framework for the \gls{mSTS}. The deployed system proved to be a reliable solution and ensured the safety of the detector for almost 1.5 years. Moreover, the data related to the performance of the detector modules were analyzed and significant progress in the quality of modules is observed. Obtained data was also used to estimate the total fluence, which was based on the leakage current changes. 


\newpage